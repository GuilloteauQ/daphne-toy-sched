#+TITLE: Notes
#+AUTHOR: Quentin Guilloteau

* <2023-08-02 Wed>

- Packaged daphne in Nix for better reproducibility: https://github.com/GuilloteauQ/daphne-nix
- to avoid rebuild everything all the time, i also set up a binary cache at https://daphne-nix.cachix.org
- the packaging is a bit dirty still, but good enough for simple tests
  - some of the deps are not managed (like cuda, mpi, fpga stuff)

Starting to look at the delivrable D5.2

- csv files are big, let's put them on zenodo
  - https://zenodo.org/record/8208151
- let's also start some snakemake boilerplate to automatize all of this
- let's also change the daphne script
  - we increase the `maxi` variable which seems to represent the number of iterations of the algorithm
  - we also add an argument to have several iterations

 seems to work fine

 #+BEGIN_EXAMPLE
 daphne --vec --num-threads=12 --select-matrix-representations --args f=\"./data/Amazon0601_0.csv\" --args iterations=10 src/components_read.daphne
 #+END_EXAMPLE

 Let's do some simple evaluation.

 Let's look at the scaling of different scheduling strategies by increasing the number of threads.

 We can actually play with all the scheduling policies.

 Let's:
 - pin the threads (--pin-workers)
 - use the centralized work queue (--queue_layout=CENTALIZED)
   - it is the default, but let's be explicit anyway
   - thus no workstealing
 - scheduling policies:
   - STATIC, SS, GSS, TSS, FAC2, TFSS, FISS, VISS, PLS, MSTATIC, MFSC, PSS
 - as i will run it on Grid'5000 in the end, let's say that we will use between 1 and 64 threads
 - as this is a small task and not a real research work, i'll just repeat 10 times each experiment


ah! small question:
- as i added a loop in my daphne script, does the distribution of work changes compared to running 10 times the same script ?
  - i suppose that the partitionning is done much lower (ie matrix mult)

ok, i added some R scripts for simple visualizations.

we will also remove SS from the policies as i dont really want to spend an eternity running the script ^^


Ok, let's now look at this custom scheduler.

The task is:

#+BEGIN_EXAMPLE
Edit the source code
 to add a new simple scheduling technique (of your choice) allowing end users to determine a fixed task size that is scheduled whenever a CPU thread is available and idle.
#+END_EXAMPLE

From this I understand that in the end we should have something like:


 #+BEGIN_EXAMPLE
 daphne --vec --num-threads=12 --select-matrix-representations --partitioning=CST --args f=\"./data/Amazon0601_0.csv\" --args iterations=10 src/components_read.daphne
 #+END_EXAMPLE

 where `CST` would stand for "constant".
The question is now to know how to pass this constant task size.

In the other scheduling strategies there are no such parameters.

The initialization is done via

#+BEGIN_SRC c++
// src/runtime/local/vectorized/LoadPartitioning.h
  LoadPartitioning(int method, uint64_t tasks, uint64_t chunk, uint32_t workers, bool autochunk) {
  // ...
  }
#+END_SRC

The method is the desired partitionining (would be `CST` for us).
`tasks` is the number of tasks.
`chunk` seems to be the minimum chunk size (remember `--grain-size`). by default it is 1.
`workers` is the number of workers.
`autochunk` seems to be if there should be chunking or not ?

one easy modification would be to pass the fixed size through the `--grain-size` flag.

it might look something like this:

#+BEGIN_EXAMPLE
daphne --vec --num-threads=12 --select-matrix-representations --partitioning=CST --grain-size=42 --args f=\"./data/Amazon0601_0.csv\" --args iterations=10 src/components_read.daphne
#+END_EXAMPLE

We might think that there might be issues with the "last few tasks" in the end if this fixed size does not divide the number of tasks.
But the current code already take care of this:

#+BEGIN_SRC c++
// src/runtime/local/vectorized/LoadPartitioning.h
  chunkSize = std::min(chunkSize, remainingTasks);
#+END_SRC


another option would be to pass the fixed tasks size through environment variable, like what is already done to override partitionning method:

#+BEGIN_SRC c++
// src/runtime/local/vectorized/LoadPartitioning.h
  if(const char* env_m = std::getenv("DAPHNE_TASK_PARTITION")) {
    method= getMethod(env_m);
  } 
#+END_SRC

So it might look like:


 #+BEGIN_EXAMPLE
 DAPHNE_CST_TASK_SIZE=42 daphne --vec --num-threads=12 --select-matrix-representations --partitioning=CST --args f=\"./data/Amazon0601_0.csv\" --args iterations=10 src/components_read.daphne
 #+END_EXAMPLE


 
It seems better to pass arguments through the env variables.
Giving several meaning to `--grain-size` would be confusing, and modifying all the calls to `LoadPartitionning` too messy.


So the diff looks like:

#+BEGIN_SRC diff
diff --git a/src/api/internal/daphne_internal.cpp b/src/api/internal/daphne_internal.cpp
index 5165e080..0bf91f2e 100644
--- a/src/api/internal/daphne_internal.cpp
+++ b/src/api/internal/daphne_internal.cpp
@@ -145,7 +145,8 @@ int startDAPHNE(int argc, const char** argv, DaphneLibResult* daphneLibRes, int
                 clEnumVal(PLS, "Performance loop-based self-scheduling"),
                 clEnumVal(MSTATIC, "Modified version of Static, i.e., instead of n/p, it uses n/(4*p) where n is number of tasks and p is number of threads"),
                 clEnumVal(MFSC, "Modified version of fixed size chunk self-scheduling, i.e., MFSC does not require profiling information as FSC"),
-                clEnumVal(PSS, "Probabilistic self-scheduling")
+                clEnumVal(PSS, "Probabilistic self-scheduling"),
+                clEnumVal(CST, "Fixed size tasks. Pass the size via the env variable DAPHNE_CST_TASK_SIZE")
             ),
             init(STATIC)
     );
diff --git a/src/parser/config/ConfigParser.h b/src/parser/config/ConfigParser.h
index 0a1021c7..85d2eb06 100644
--- a/src/parser/config/ConfigParser.h
+++ b/src/parser/config/ConfigParser.h
@@ -35,7 +35,8 @@ NLOHMANN_JSON_SERIALIZE_ENUM(SelfSchedulingScheme, {
     {PLS, "PLS"},
     {MSTATIC, "MSTATIC"},
     {MFSC, "MFSC"},
-    {PSS, "PSS"}
+    {PSS, "PSS"},
+    {CST, "CST"}
 })

 class ConfigParser {
diff --git a/src/runtime/local/vectorized/LoadPartitioning.h b/src/runtime/local/vectorized/LoadPartitioning.h
index d1fd2c5b..892cc6a9 100644
--- a/src/runtime/local/vectorized/LoadPartitioning.h
+++ b/src/runtime/local/vectorized/LoadPartitioning.h
@@ -21,6 +21,7 @@
 #include <cmath>
 #include <cstdlib>
 #include <string>
+#include <iostream>

 class LoadPartitioning {

@@ -35,6 +36,7 @@ private:
     uint64_t tssChunk;
     uint64_t tssDelta;
     uint64_t mfscChunk;
+    uint64_t cstChunk;
     uint32_t fissStages;
     int getMethod (const char * method){
         return std::stoi(method);
@@ -75,6 +77,16 @@ public:
         tssChunk = (uint64_t) ceil((double) totalTasks / ((double) 2.0*totalWorkers));
         uint64_t nTemp = (uint64_t) ceil(2.0*totalTasks/(tssChunk+1.0));
         tssDelta  = (uint64_t) (tssChunk - 1.0)/(double)(nTemp-1.0);
+        cstChunk = 0;
+        if (schedulingMethod == CST){
+            if (const char* env_cst_size = std::getenv("DAPHNE_CST_TASK_SIZE")){
+                cstChunk = std::stoi(env_cst_size);
+            }
+            else{
+                std::cerr << "Env. variable DAPHNE_CST_TASK_SIZE not set! Falling back on MFSC" << std::endl;
+                schedulingMethod = MFSC;
+            }
+        }
     }
     bool hasNextChunk(){
         return scheduledTasks < totalTasks;
@@ -142,6 +154,10 @@ public:
                 chunkSize=mfscChunk;
                 break;
             }
+            case CST:{
+                chunkSize=cstChunk;
+                break;
+            }
             default:{
                 chunkSize = (uint64_t)ceil(totalTasks/totalWorkers/4.0);
                 break;
diff --git a/src/runtime/local/vectorized/LoadPartitioningDefs.h b/src/runtime/local/vectorized/LoadPartitioningDefs.h
index d0b66eff..c2f10f2d 100644
--- a/src/runtime/local/vectorized/LoadPartitioningDefs.h
+++ b/src/runtime/local/vectorized/LoadPartitioningDefs.h
@@ -42,5 +42,6 @@ enum SelfSchedulingScheme {
     MSTATIC,
     MFSC,
     PSS,
+    CST,
     INVALID=-1 /* only for JSON enum conversion */
 };
#+END_SRC

The modified version of daphne is in the `daphne-cst-shell` nix shell

We can see that the policy is available

#+BEGIN_EXAMPLE
bash-5.2$ daphne --help
OVERVIEW: The DAPHNE Prototype.

This program compiles and executes a DaphneDSL script.

USAGE: daphne [options] script [arguments]

OPTIONS:

Advanced Scheduling Knobs:

  --debug-mt                 - Prints debug information about the Multithreading Wrapper
  --grain-size=<int>         - Define the minimum grain size of a task (default is 1)
  --hyperthreading           - Utilize multiple logical CPUs located on the same physical CPU
  --num-threads=<int>        - Define the number of the CPU threads used by the vectorized execution engine (default is equal to the number of physical cores on the target node that executes the code)
  --partitioning=<value>     - Choose task partitioning scheme:
    =STATIC                  -   Static (default)
    =SS                      -   Self-scheduling
    =GSS                     -   Guided self-scheduling
    =TSS                     -   Trapezoid self-scheduling
    =FAC2                    -   Factoring self-scheduling
    =TFSS                    -   Trapezoid Factoring self-scheduling
    =FISS                    -   Fixed-increase self-scheduling
    =VISS                    -   Variable-increase self-scheduling
    =PLS                     -   Performance loop-based self-scheduling
    =MSTATIC                 -   Modified version of Static, i.e., instead of n/p, it uses n/(4*p) where n is number of tasks and p is number of threads
    =MFSC                    -   Modified version of fixed size chunk self-scheduling, i.e., MFSC does not require profiling information as FSC
    =PSS                     -   Probabilistic self-scheduling
    =CST                     -   Fixed size tasks. Pass the size via the env variable DAPHNE_CST_TASK_SIZE
  --pin-workers              - Pin workers to CPU cores
  --pre-partition            - Partition rows into the number of queues before applying scheduling technique
  --queue_layout=<value>     - Choose queue setup scheme:
    =CENTRALIZED             -   One queue (default)
    =PERGROUP                -   One queue per CPU group
    =PERCPU                  -   One queue per CPU core
  --vec                      - Enable vectorized execution engine
  --victim_selection=<value> - Choose work stealing victim selection logic:
    =SEQ                     -   Steal from next adjacent worker (default)
    =SEQPRI                  -   Steal from next adjacent worker, prioritize same NUMA domain
    =RANDOM                  -   Steal from random worker
    =RANDOMPRI               -   Steal from random worker, prioritize same NUMA domain

DAPHNE Options:

  --args=<string>            - Alternative way of specifying arguments to the DaphneDSL script; must be a comma-separated list of name-value-pairs, e.g., `--args x=1,y=2.2`
  --config=<filename>        - A JSON file that contains the DAPHNE configuration
  --cuda                     - Use CUDA
  --distributed              - Enable distributed runtime
  --enable-profiling         - Enable profiling support
  --explain=<value>          - Show DaphneIR after certain compiler passes (separate multiple values by comma, the order is irrelevant)
    =parsing                 -   Show DaphneIR after parsing
    =parsing_simplified      -   Show DaphneIR after parsing and some simplifications
    =sql                     -   Show DaphneIR after SQL parsing
    =property_inference      -   Show DaphneIR after property inference
    =select_matrix_repr      -   Show DaphneIR after selecting physical matrix representations
    =phy_op_selection        -   Show DaphneIR after selecting physical operators
    =type_adaptation         -   Show DaphneIR after adapting types to available kernels
    =vectorized              -   Show DaphneIR after vectorization
    =obj_ref_mgnt            -   Show DaphneIR after managing object references
    =kernels                 -   Show DaphneIR after kernel lowering
    =llvm                    -   Show DaphneIR after llvm lowering
  --fpgaopencl               - Use FPGAOPENCL
  --libdir=<string>          - The directory containing kernel libraries
  --no-ipa-const-propa       - Switch off inter-procedural constant propagation
  --no-obj-ref-mgnt          - Switch off garbage collection by not managing data objects' reference counters
  --no-phy-op-selection      - Switch off physical operator selection, use default kernels for all operations
  --select-matrix-repr       - Automatically choose physical matrix representations (e.g., dense/sparse)
  --timing                   - Enable timing of high-level steps (start-up, parsing, compilation, execution) and print the times to stderr in JSON format

Distributed Backend Knobs:

  --dist_backend=<value>     - Choose the options for the distribution backend:
    =MPI                     -   Use message passing interface for internode data exchange
    =gRPC                    -   Use remote procedure call for internode data exchange (default)

Generic Options:

  --help                     - Display available options (--help-hidden for more)
  --help-list                - Display list of available options (--help-list-hidden for more)
  --version                  - Display the version of this program

EXAMPLES:

  daphne example.daphne
  daphne --vec example.daphne x=1 y=2.2 z="foo"
  daphne --vec --args x=1,y=2.2,z="foo" example.daphne
  daphne --vec --args x=1,y=2.2 example.daphne z="foo"
#+END_EXAMPLE

We can use it with the environment variable

#+BEGIN_EXAMPLE
bash-5.2$ DAPHNE_CST_TASK_SIZE=42 daphne --vec --num-threads=12 --select-matrix-representations --partitioning=CST --args f=\"./data/Amazon0601_0.csv\" --args iterations=1 src/components_read.daphne
5.38057
#+END_EXAMPLE

And if we do not use it we fall back on MFSC, but with a error message.

#+BEGIN_EXAMPLE
bash-5.2$ daphne --vec --num-threads=12 --select-matrix-representations --partitioning=CST --args f=\"./data/Amazon0601_0.csv\" --args iterations=1 src/components_read.daphne
Env. variable DAPHNE_CST_TASK_SIZE not set! Falling back on MFSC
Env. variable DAPHNE_CST_TASK_SIZE not set! Falling back on MFSC
Env. variable DAPHNE_CST_TASK_SIZE not set! Falling back on MFSC
Env. variable DAPHNE_CST_TASK_SIZE not set! Falling back on MFSC
Env. variable DAPHNE_CST_TASK_SIZE not set! Falling back on MFSC
Env. variable DAPHNE_CST_TASK_SIZE not set! Falling back on MFSC
Env. variable DAPHNE_CST_TASK_SIZE not set! Falling back on MFSC
Env. variable DAPHNE_CST_TASK_SIZE not set! Falling back on MFSC
Env. variable DAPHNE_CST_TASK_SIZE not set! Falling back on MFSC
Env. variable DAPHNE_CST_TASK_SIZE not set! Falling back on MFSC
Env. variable DAPHNE_CST_TASK_SIZE not set! Falling back on MFSC
Env. variable DAPHNE_CST_TASK_SIZE not set! Falling back on MFSC
Env. variable DAPHNE_CST_TASK_SIZE not set! Falling back on MFSC
Env. variable DAPHNE_CST_TASK_SIZE not set! Falling back on MFSC
Env. variable DAPHNE_CST_TASK_SIZE not set! Falling back on MFSC
Env. variable DAPHNE_CST_TASK_SIZE not set! Falling back on MFSC
Env. variable DAPHNE_CST_TASK_SIZE not set! Falling back on MFSC
Env. variable DAPHNE_CST_TASK_SIZE not set! Falling back on MFSC
Env. variable DAPHNE_CST_TASK_SIZE not set! Falling back on MFSC
Env. variable DAPHNE_CST_TASK_SIZE not set! Falling back on MFSC
Env. variable DAPHNE_CST_TASK_SIZE not set! Falling back on MFSC
Env. variable DAPHNE_CST_TASK_SIZE not set! Falling back on MFSC
Env. variable DAPHNE_CST_TASK_SIZE not set! Falling back on MFSC
Env. variable DAPHNE_CST_TASK_SIZE not set! Falling back on MFSC
Env. variable DAPHNE_CST_TASK_SIZE not set! Falling back on MFSC
Env. variable DAPHNE_CST_TASK_SIZE not set! Falling back on MFSC
Env. variable DAPHNE_CST_TASK_SIZE not set! Falling back on MFSC
Env. variable DAPHNE_CST_TASK_SIZE not set! Falling back on MFSC
Env. variable DAPHNE_CST_TASK_SIZE not set! Falling back on MFSC
Env. variable DAPHNE_CST_TASK_SIZE not set! Falling back on MFSC
Env. variable DAPHNE_CST_TASK_SIZE not set! Falling back on MFSC
Env. variable DAPHNE_CST_TASK_SIZE not set! Falling back on MFSC
Env. variable DAPHNE_CST_TASK_SIZE not set! Falling back on MFSC
Env. variable DAPHNE_CST_TASK_SIZE not set! Falling back on MFSC
Env. variable DAPHNE_CST_TASK_SIZE not set! Falling back on MFSC
Env. variable DAPHNE_CST_TASK_SIZE not set! Falling back on MFSC
Env. variable DAPHNE_CST_TASK_SIZE not set! Falling back on MFSC
Env. variable DAPHNE_CST_TASK_SIZE not set! Falling back on MFSC
3.4608
#+END_EXAMPLE


Why there are so many error messages ?
there are 12 threads, and a single iteration.


Ok, gotta go, will continue a bit tomorrow.

* <2023-08-03 Thu>

Add some details to the readme.

did a quick evaluation of the `CST` scheduler.

You can run it with:

#+BEGIN_SRC
nix develop --command snakemake -c1 plots/cst.pdf
#+END_SRC

Don't forget that you might want to change some of the parameters of the experiments in `config/config.yaml`

As expected, too small task size results in long compute time (because of task creation overhead) and large tasks result in also long compute time (unbalanced).


Also, why did i decided to use mFSC as a fallback ?
let's use static, at it the default sched policy


Now some reading

** D5.1 Scheduler design for pipelines and tasks

- program seen as a DAG of operators
- daphne compiler can rewrite operations
  - which op to execute
  - in which order
  - and where (CPU, GPU, FPGA, etc.)
- daphne compiler also decides if parallelization is required
- can control work granularity
  - by changing chunk sizes
    - one chunk, several tasks
  - by changing tasks sizes
    - one chunk, one task, but a big one
- several DLS (Dynamic Loop Sel-scheduling) policies
  - Non adaptive 
    - *Static* - one big task per worker
    - *SS* - task size = 1
    - *Fixed Size Self-scheduling (FSC)* - makes hyp on knowing the variability in iteration exec time and sched overhead.
      - *mFSC* - more practical as it doesnt require those info
    - *Guided Self-scheduling (GSS)* - decreasing chunk sizes 
    - *Trapezoid SS (TSS)* - same as GSS with linear function to decrease chunk size
    - *Factoring (FAC)* - assume knowledge of mean and std dev execution time of tasks
      - *FAC2* - more practival, assign half of the remaining tasks at each round
    - *Trapezoid factoring (TFSS)* - FAC + TSS
  - adaptive
    - *PLS* - 1st part of the work scheduled statically, then the 2nd part is scheduled dynamically with GSS
      - the ratio of work for each part is determined with the Static Workload Ratio (SWR) equal to the min iter exec time divided the max iter exec time
    - *PSS* - schedule the work based on the remaining work and the number of processors expected to be available in the future
    - *AWF* - variable sized chunked given weights. these weights can be updated at runtime based on processor performance
      - several variants (AWF-B, AWF-C, AWF-D, AWF-E)
	- reminded me of this hehe https://www.youtube.com/watch?v=2gLumNRmqjs
      - I should investigate more about those
    - *AF* - based on FAC but learns the mean and std dev values at runtime
- also distributed scheduling
  - two levels of scheduling
- also NUMA-aware scheduling
- want to also have a cluster-level scheduler
- and *resource sharing*
  - TODO: go read some of the papers cited
    - https://dl.acm.org/doi/pdf/10.1145/2155620.2155650
  

As I understand, in a distributed context the work queue can be centralized or decentralized.
In the case of decentralized, when the local queue is empty, the local instance must fetch some new tasks from the centralized queue.
So this looks like a regulation problem.
You could see it the other way, where the central queue is sending tasks to the local queue to instead of the local workers querying the central one.
This then looks (from a biased eye) like a regulation problem.
The question being: How to maintain enough work in the local queues to avoid starvation and balance load ?
It looks like queue theory stuff.
